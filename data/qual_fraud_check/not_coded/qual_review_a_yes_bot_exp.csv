ResponseId,qual_review,has_edu_email,qual_compensation,qual_media,qual_phase,qual_impact,qual_tactics,qual_recommend,qual_ethics,qual_resources2,qual_open
R_3KNtkDdSgdYPPrb,A,1,slightly - we had been worried about bots and so set up screening surveys,Facebook,# of responses to survey within short # of time,made recruitment much harder,NA,Needing to use multiple approaches to detect and avoid bots,Generally leaned in favor of removing the suspected bots,more $ to use recruitment services,so difficult to avoid; captcha did not seem to help
R_2qlsjA1vbSaZMrc,A,1,"Bots have not yet impacted my stance on compensation, but in hindsight, I did end up compensating a lot of bots. In future studies I plan to be more careful about reviewing tasks before compensating to minimize the likelihood of compensating bots.",N/A,I noticed impossibly fast study completion times,I decided to not use approximately a third of the data I had collected as I suspected the data was fraudulent,NA,NA,"One way that I attempted to navigate this balance was by flagging suspicious cases and then comparing groups of suspicious vs. non-suspicious responders on all study variables. After finding significant differences between the groups on the study variables, I felt more confident in my decision to remove the suspicious data altogether.",More education/literature on novel quality checks that can be incorporated into surveys.,"I used the CloudResearch MTurk Toolkit, an application programming interface that utilizes MTurk. I specifically opted to use this interface because CloudResearch advertises that they screen for only high quality participants. Unfortunately, I still had a significant issue with bots despite this additional check."
R_2fUBGotFFghL5p7,A,1,It has not,Mturk,‚ÄúRed herring‚Äù questions,It ruined the entire dataset,None,This was the second study I did using mturk. The first time was two years earlier and I got quality date. But it changed very quickly. I switched the study to prolific but had to change the entire design and sample.üòû,With that study I had to throw out the entire dataset. I only ended up having less than 50 potential human respondents (12.5% of original data). Even that data was questionable.,Looking for other alternatives‚Ä¶,Check before approving compensation. Although this has a serious impact on another study. Still figuring it out.
R_2ZEMMv7mH8bZhsz,A,1,I use tactics to screen out the bots. It does mean occasionally that someone who did complete a large part of the study but was not paying good attention gets kicked out and is upset.,NA,Reading the information on the web.,"The kind that fail the various tools to check (disguised attention checks, Rcapture)","RCapture and similar; attention check example ""I am no longer alive"" which should be answered false.","build in hard to find attention checks (if you struggle to catch them all when going through the study and trying to not read, they are probably pretty good); use tools that require evaluating picture images","this is a risk. However, in addition to bots, non-attentive humans is an issue.",NA,NA
R_2VlKJeWS9JVyAv9,A,1,None,"Facebook and Instagram ads, flyers in the community, emails, SONA,","A ton of new ""people "" took the screening survey but when i called to schedule the experiment the phone numbers were all not in service",Extra time it took to remove and determine which ones were bots,NA,NA,Calling the phone numbers on the screener,Common procedures,NA
R_1I4ptPZHaAPAOpE,A,1,NA,Facebook,NA,NA,NA,NA,NA,NA,NA
R_2zJeEGJBRq6L8wR,A,1,If we were unsure we would just pay them,facebook ad,"responses didn't make sense, provided weird responses in open ended questions",We usually removed them during data cleaning bit it definitely impacted sample size and power,NA,You cannot rely on 1or 2 ways to detect bots. You must implement several methods and evaluate them as a whole,We only removed them if they fit multiple criteria of bots. If they fit 1-2 or we are unsure we always retain them,"programming genius, more staff and resoures to verify participants before study entry",NA
R_2zioEpueKCHMtXy,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_3P63jqlUz4I01eC,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_vj1IRLw9cxzQz05,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_3OieFyFdNjyzhsG,A,1,NA,Facebook,NA,NA,NA,NA,NA,NA,NA
R_2s5SWbhfq6Od9yl,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_dmcdd6SXVocJUeB,A,1,NA,Facebook,NA,NA,NA,NA,NA,NA,NA
R_Rt1Qg8ol7fpNOdH,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_737JTsqFzHbfsNr,A,1,NA,Instagram,NA,NA,NA,NA,NA,NA,NA
R_2TzKD8ZHWSRBboO,A,1,NA,Public groups,NA,NA,NA,NA,NA,NA,NA
R_3oA4AW6XeWsfge3,A,1,NA,Private direct messaging,NA,NA,NA,NA,NA,NA,NA
R_324pRl5s2blybL3,A,1,NA,Facebook,NA,NA,NA,NA,NA,NA,NA
R_1eQuUvsuoiVo3XZ,A,1,NA,Facebook,NA,NA,NA,NA,NA,NA,NA
R_1hHgk10TXCpFlzx,A,1,none,"GroupMe, Reddit, Discord, Instagram, Facebook, and Snapchat",answers didn't make sense,bogus cases,NA,NA,we'd rather err on the side of excluding a human vs including a bot,"now that we are alert to this, honeypot questions etc seem to have sufficed",good luck with your research
R_3Krnht3Ym93ixsF,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_Z2DluCNzRiL12gN,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_1kXpuzOkdVqtAAQ,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_2e9UVKjadHYxP8O,A,1,Participated in multiple surveys.,Facebook,The same ip appears.,Exacerbated my workload.,Verification of biological information.,NA,Only half of all data is used.,Enhanced verification of biological information.,NA
R_1IQz6Gav1iwSVej,A,1,NA,Facebook Twitter,cardiovascular and cerebrovascular diseases,Free oneself,Improve efficiency and increase interactivity,NA,NA,NA,NA
R_301YjsMUAnntAt6,A,1,NA,Twitter,NA,NA,NA,NA,NA,NA,NA
R_1f8fwWAXLijZNLo,A,1,NA,private direct messaging),NA,NA,NA,NA,NA,NA,NA
R_2Et8Ihrn7ajDM5k,A,1,NA,Facebook,NA,NA,NA,NA,NA,NA,NA
R_YaiNmjPhsRtCIX7,A,1,NA,public groups,NA,NA,NA,NA,NA,NA,NA
R_AoGHGEgqd0RxL57,A,1,"Robots have always been easy to identify if you look at them carefully, I don't usually care",Group addition,psychopathology,estrangement,See if he's on the same page as me,Ignore the robot. The robot will ask you questions in technical terms,no,Big data,no
R_TvK3PgQrKYOw0oh,A,1,NA,private direct messaging,NA,NA,NA,NA,NA,NA,NA
R_sdneouS1JwWQjHb,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_1JEKNwY0ijiimg9,A,1,NA,public groups,NA,NA,NA,NA,NA,NA,NA
R_1fmefIVLoIIrdhy,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_1NaVE0yDAndWUL7,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_p5Vmh4H36LZgsDL,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_2ql9vxLhrFqSs8u,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_3kdLv2vsRjCkJkw,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_br2IObWHlPG6Ah3,A,1,Working hours,Twitter,The data analysis of the study is problematic.,Bad influence,Verify with pictures,Understand the logic of robot thinking and make effective response.,Through effective means.,Through some software.,Nothing
R_3nNTeCIl7U1fJVQ,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_xFam2StWnmMkyaJ,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_qQwJJ3AJ3l23Yc1,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_3kv3EzfY4K5etbe,A,1,"I have only encountered bots through mTurk. At first they would fail participant checks, which was fine; eventually however, it was clear that the data were junk but participant checks were still being passed. I wound up spending money on useless data; I stopped using mTurk.",n/a just mTurk,Sudden increase in participants; all participants coming from India (notably despite U.S. requirements),"Ruined data collection, wasted money",NA,Don't use mTurk; identify credible online recruitment methods.,This is a major reason why I threw the data out and started over - more than once.,More credible sources of data.,"I have had colleagues advertise studies on social media as well, that got majorly overtaken by bots; it feels as though the research endeavor is very difficult to do well right now, especially if you want to recruit beyond college students."
R_u34Axg8ltFwLbOh,A,1,Bots did not impact compensation. I had a screening form set up that people who were interested would complete and if eligible they would provide an email that I would reach out to. Bots did not provide an email during those spamming periods.,"Instagram, relevant Facebook groups, Twitter, Reddit, LinkedIn",I noticed ballot stuffing in my screening form by the hundreds,"Luckily, a statistician I'm working with is well-known for their bot mitigation efforts. So they provided guidance from the survey design stage where I included a screening form for eligibility and bot detection purposes, so that protected my data of the full survey.",Screening form to determine eligibility and fraud/ballot stuffing scores,NA,The screening form has captured the bots since I was able to identify the patterns in the data above. I checked the quantitative and qualitative full surveys that are sent directly to the participants who did not flag on the screening forms. Their answers did not appear to be bot-like after analyzing the known patterns of bot responses.,"Screening forms are amazing! In addition, skip logics, ballot stuffing and fraud detection mechanisms within the surveys are very helpful.","They take a lot of time to remove from screening form data too when ballot stuffing occurs, which just adds time to the data collection process."
R_2zD5YcFIn4Lukf9,A,1,None.,N/A,"Repeated or very similar IP addresses, dubious geolocation","Very limited. I identified them quickly, reported them, and took their ID down so I won‚Äôt invite them back to any of my lab studies in the future",I used those outlined above,NA,Used multiple ways to detect bits and deleted data points only if they failed 60% of my checks,I have the resources I need,There is not
R_3n6qih6lvBbfkaI,A,1,We now don‚Äôt pay for baseline,NA,NA,NA,NA,NA,NA,NA,NA
R_3mlqMnLjBj9drmR,A,1,"When the compensation survey is disconnected from the main survey to preserve anonymity, it is hard to know if bots are being compensated.",Facebook public and private groups; posts on Instagram and twitter,Free response answers were very strange and analyses revealed suspect response patterns.,"They made data ""cleaning"" and analysis annoying.",NA,Include attention/effort check questions; include open-ended/write-in questions; link incentive surveys to main surveys so that main surveys have to be completed in order to access the incentive survey; add a CAPTCHA; include multiple questions that can be compared for consistency; use R code or other scripts to check for effortful/attentive response patterns; create a protocol for data integrity checks and follow it when preparing data for analysis; etc.,I construct multi-point integrity check protocols that take several data points into account when selecting cases for removal.,"Ways to vet participants. I maintain a list of emails that I collect at pride festivals that is slowly growing. Given that I have collected those emails from in-person contacts, I consider them fairly reliable and hope to rely on them more for data collection in the future.",I appreciate you doing this work and I look forward to learning from your findings.
R_1gCmU5ldvy2LXK8,A,1,It has not impacted our decision to offer compensation,None--it was all done through email communications to students coming from the college,There was a drastic increase in the amount of data that we received from one institution,We had to modify our recruitment and allow multiple verifications that the participants were real people and not bots,Captcha; bot detection on Qualtrics,Scan literature for the latest in best practices for detecting bots in online research; expect that you will encounter them if you are compensating participants,"It has been easy for us to determine data produced by bots, but if there is any question, we bring cases to our research meetings so that the group can weigh in","software that can detect bots in progress, kick them out of the survey, and prevent that system from re-accessing the survey",NA
R_1QAian2BoPFFSJK,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_cVnjO5P2biXNBSN,A,1,Robots greatly reduce fairness,"Twitter, Facebook",Obvious data bias,"The impact is too large, confusing the truth and accuracy of the data.",There are no particularly good measures,There are no particularly good measures to share,"There is no particularly good way, don't want to delete the real investigators",It would be great to have software that effectively removes robots,NO
R_5yZOYCdcdDcOXoR,A,1,Did not impact compensation,NA,Response times,Lowered sample size,"Captcha verification, evaluating impossible answers (for example, reporting country as US and then choosing a Canadian province instead of a US state)",NA,Cut off response time and other methods to remove bots meant that if a real person answered the data was still low quality,"More steps and research assistant time. For crowdsourcing, potentially using other services. This was a unique study for me (one cross sectional survey) as I usually do longitudinal studies where I would take the time to confirm through emails/calls.",NA
R_2ZQAy2XgHKT2DKb,A,1,"In one study in which we were compensating everyone who participated, we had to do lots of data cleaning and determine who we would be pay. Now, I ensure that even for raffles, I get an email acknowledgment from a participant before sending payment.",Facebook ads on Facebook and Instagram (FB posts them across both platforms) and ads on Scruff.,"In a very brief time, we received upwards of 80-100 responses to our screener questionnaire. Upon inspection, found IP address issues (multiple entries from same IP), suspicious email addresses, etc.","Had to clean the bots out of screener so that we had accurate count of eligibility. Because the study was an RCT conducted in person or virtual, would have not included bots in the actual RCT or data collection of pre-, post-, or follow-up data, but it did impact the eligibility data.",Not putting amount of compensation in recruitment materials.,Have a strategy from the get-go for addressing bots. There are several recent publications that provide recommendations. Include the bot strategy in your IRB application.  Always check for bots.,I feel pretty confident in who was removed and real people were removed by mistake I'd rather that than the bias from bots.,More publications on best practices. Please publish your work!!,NA
R_2S7UfMHLmB09L6B,A,1,"We need to make compensation contingent upon data integrity review. So our consent form states ""You may receive $50 Amazon gift card for your time taking the online survey. You may only complete the survey once. Participant IP addresses will be collected in Qualtrics after the survey is complete for data quality purposes. IP addresses will be discarded at the conclusion of the study. If responses to the survey are found to be inconsistent (e.g., bots, en mass responses, etc.), ineligible, multiple responses are made by the same user, or an individual is found to be purposely manipulating the survey (e.g., participating in a survey for which they are not eligible, providing inconsistent responses, etc.), payment will be withheld.""","Instagram, Twitter, Facebook","Massive response with a very high number of seemingly eligible respondents (over 4,000 screens, nearly 3,000 appeared eligible). Also noticed similarly formatted emails, in rapid succession.","We hope that we removed most of the bots, but don't have a good sense of the sensitivity and specificity of methods used to remove them from the final sample. We used a variety of methods including Qualtrics RelevantID features, as well as manual checks (e.g., comparison questions). We're currently preparing a manuscript outlining the methods used and comparing the ""bot"" sample to our final ""valid"" sample. We did have bots email the study and the university IRB after they did not receive compensation. The emails were always strangely formatted and had awkward grammar.",Qualtrics RelevantID,NA,"It is very difficult to know the sensitivity and specificity of our methods - I spent hours and hours thinking about this, and still do. We used a ""flag"" system where respondents would get up to 1 pass, 2 or more flags were removed. We're writing a manuscript describing special considerations for marginalized populations, like shared devices/wi-fi connection, etc. Even after all the checks, there were some participant who appeared eligible who were only identified as a ""survey farmer"" when invited for a Zoom interview, appeared on camera, and then were apparently not the population of focus of the study.","Luckily, we anticipated bots so were able to build in a lot of this in the IRB application initially. I worry about other research that has perhaps had bots but did not know to look for them.",NA
R_Y3aUDAZSnkg9Rnz,A,1,"Hopefully in no ways but unfortunately I'm unsure if this is the case (e.g., if I didn't thoroughly clean my data and a bot was awarded a raffle gift card).","Facebook, Twitter, Reddit (likely the biggest source of bots), professional listservs that focused on relevant issues to the study, word of mouth to friends/colleagues not on social media",Large number of responses/participants were recorded overnight (mostly 2-4am). Normally the study only recorded about 1 participant every week or so.,Required extra time to clean data (on top of normal protocols). Potentially resulted in a missed/overlooked bot getting awarded a randomly-selected raffle reward.,NA,NA,"Cleaned data in stages (did preliminary cleaning, review this initial cleaning days later, once later before final data analysis, etc.). Recruited colleague to do parallel cleaning and then run inter-rater reliability check. Retained all suspected bots in data set and created variable to denote suspicion of bots (e.g., 1 = Not Bot, 2 = Suspicious, 3 = Confident is Bot) and then run analyses comparing final results based on whether sample included bots, suspicious, etc.","Unsure - future discussion & recommendations would be helpful. Also, none of my methods courses in graduate school/training covered this, so future cohorts of folks would likely benefit from programs directly discussing this as part of curriculum.",NA
R_1IsmZP4X8dt4tQQ,A,1,Increases cost of screening.,none,failed bot check items.,none other than time required to prevent and remove.,NA,"Do everything you can in terms of current tech, but always be sure to carefully examine the data.",use the same algorithm when deciding for all cases.,creative tools to catch them.,they are a feature of the system.
R_1DRTfCig2DPcPhj,A,1,No effect on compensation (to my knowledge) because I have a screening protocol that I hope works.,Prolific Academic,Knew it was an issue beforehand,Requires more work during the compensation/screening stage of using Prolific. Need to actually have a process for identifying them and some bot farms have people who will message you and fight you when you don‚Äôt comp them. One message back to them re: how you know they used a bot to complete it usually sends them away and you can save the $ to comp an actual participant.,NA,"Create your screening protocol beforehand so that you have honest grounds to not compensate ‚Äúparticipants‚Äù. Take the time to actually enforce these standards and be open to the fact that some bots will get through. Mixed methods help screen them out, especially the newer questions that Qualtrics can support like heat maps, etc. Be innovative and you‚Äôll outsmart the bots because someone has to first encounter something in a survey in order to program a bot to bypass it.","Always open to the responses of participants on PA when I send the rejection email. Most bots don‚Äôt respond, the ones that do will use egregious tactics to get the money (guilting you about how that money is the line between life or death for them ‚Äî a very common pattern I‚Äôve encountered running hundreds of thousands of participants on PA). But sometimes you think a person is a bot and they‚Äôre not! And their message might convince you.",Better getting from recruitment panels on the beginning end (uploading/checking IDs).,it‚Äôs an issue but the ability to recruit any sample/demographic in an instant is an amazing technological feat that we should be incredibly thankful for and in awe of. We can totally problem solve this.
R_332U9CmMwIu70aj,A,1,"I have considered the possibility of switching to a raffle system, which I know others say cuts down on bots, but this is counter to my values of paying everyone who contributes to research. So ultimately, the bots haven't changed my compensation of participants but they have made me think more about it.","We have a large database of organizations across the US who work with LGBTQ+ individuals and contacted them to share info about the study. Also used Facebook, Twitter, Instagram; professional email listservs and other means.","We were prepared for bots to be an issue because we're familiar with the rise of this issue more generally in the field. We have an elaborate screening process for getting into our study. Participants first complete a screener questionnaire with basic demographics and contact information. For people who look legit, we go the next step of having a texting conversation with them about the study and if they seem legit then they are sent an individual link connected to their participant ID number and a unique code to access the study. During the screening process there were a lot of giveaways about bots, including duplicate IP addresses, odd names or entering first and last name when it only asks for first name, suspicious email addresses, duplication phone numbers. Also, this study was advertised as being specifically for trans and nonbinary people. The bots often entered gender and sex assigned at birth info that meant they were cisgender. This was one of the first red flags and then they usually had some combination of all the other issues noted. After individuals are enrolled into the study, we have a variety of other checks too like duplicate demographic information that is compared to their screener answers, CAPTCHAs, and others.","We have had an extremely slow enrollment. We had approximately 2,500 screener questionnaires completed and only about 21% of those came from people who looked legit and qualified for our study. This has lengthened our project significantly. Also, the amount of people power needed for texting all the individuals who qualify is significant.",A texting enrollment process after going through the screener questionnaire; unique link /code for each person; consent comprehension questions (3 questions about the content of the consent form to verify that they read it),"My main lesson is that if you have an online study, this will happen to you. I don't trust data from studies that do not include checks to verify participants or screen for bots. As a reviewer, I've become much more skeptical of online research. We need online studies to reach marginalized communities. And, we also need quality data. When researchers do nothing to address this issue, they are creating noise in the literature and it makes me question so much of what's been published.",There are very likely some real participants who were screened out of our study. I think that risk is worth it though to protect the integrity of the data collected.,Access to a texting platform and staff to manage that on a continual basis. Staff to actively review every screener that comes in to verify it looks like a legit participant's responses.,"I'll just say that some of these questions were a little hard to answer because we had a two step process of a screener questionnaire and then the actual study. I answered about the percent of bots related to the screening stage, but the figure is different and much lower once participants were actually enrolled in the study. It might be helpful to have separate questions in the future to disentangle the info for those who use this kind of two step process."
R_b3r45ZZk6ay9BAt,A,1,no,twitter„ÄÅreddit,Data analysis,Analytical behavior change,no,NA,no,no,no
R_29n27Mze0ntBmnL,A,1,(1) Impacted the decision to use gift cards that may not be used in all countries. (2) Necessitated strict exclusion criteria prior to the completion of the study.,Facebook; Twitter; Reddit; Nextdoor.com; Craigslist.com,hundreds of initiated surveys within minutes that didn't get completed.,Detecting the bots took up 90% of my attention and energy during the data collection process. It required vetting every single recruit. It shifted what I thought would be a mostly passive data collection procedure into a more active one.,Scruitinizing inconsistent answers within surveys that had some reverse-coded items; rotating the QA questions; reaching out by email to confirm email address; disallowing participation through a VPN.,NA,"For respondents that passed all of the usual checks endorsed in the previous page (e.g., QA questions, IP address check), but were suspected of being a bot, we focused on the time spent on average per question. Some bots (and humans!) answered questions too quickly for anyone who actually was reading the questions. If a respondent spent less than 1 second per question, they were either a bot or simply clicking through without reading. We adjusted the 1 second criterion depending on the survey and length of the questions.",These studies definitely require someone to have their eye on the incoming data at all time. I would need an RA whose job it was to only keep track of this. I would also need.,"We conceived of threats to the validity of our data coming from three sources: (1) humans that were clicking quickly through and not paying attention OR misrepresented themselves and were not actually eligible for our study (e.g., located outside of the U.S.), (2) bots that autonomously took the survey, and (3) ""cyborgs"" - humans that took the survey with the assistance of computer software. This last group was the most difficult to catch because they were able to get around a lot of the measures we put in place to catch them. For the cyborgs, we mostly had to look at the time spent per question in order to determine that they were not reading the questions. Given advances in the ability of software to generate human-like responses (i.e. ChatGTP) this will only get more difficult."
R_3Emqa6K3T2MqNzj,A,1,"They didn't because I removed them before I compensated. But if I had an automatic compensation process, it would be a nightmare","I did not use facebook or twitter. I did use instagram, GroupMe, WhatApp, etc.","I went from 100 responses to 2,000 responses within 48 hours",Horrible. It took ages to clean the data and then try to identify which ones were bots. The bots were good at replicating human like behavior too. So I took a more conservative approach and deleted more than I probably should so I probably had some good data that I deleted because I did not want to compromise the inttegrity of the study,"Despite thoughtful and rigid restrictions to the survey design including password protection, validity checks, prevention of multiple submissions (also known as ‚Äúballot stuffing‚Äù), and inclusion criteria, the survey still remained vulnerable to digital threats. 1481 participants failed to respond correctly to two validity check items and were subsequently eliminated from the analyses. These omitted responses included participants who provided initial consent but did not respond to any survey items (n=819). Data from participants who did not complete at least 85% of the items on the survey were removed, reducing the sample size to 812. This remaining data included a combination of authentic and inauthentic responses. Consultation with other researchers, data specialists, and emerging literature informed best practices to detect fabricated data (Dennis et al., 2020; Storozuk et al., 2020; Teitcher et al., 2015). Rigorous procedures were used to remove fraudulent data from further analysis. Text entries to an open-ended qualitative question (e.g., ‚ÄúWhat was your experience like receiving support services or therapeutic treatment?‚Äù) were inspected for duplicate responses and quality of responding. 145 responses were omitted for having identical responses to other participants. Text entries were also inspected for nonsensical or incomprehensible responses","Insert Captcha, use Qualtrics Bot detection feature (unfortunately, these things are not offered at our institutional license), have various forms of validity checks (what color is the sky?), pause data collection any time there is a sudden spike",it was hard and i probably did delete some real humans. But in my mind I wanted to be safer rather than sorry and if I needed to then I would just have to pause data collection and recruit more real participants in a second wave of data collection,"CAPTCHA (the confirm u are not a bot feature or the click all the photos with cars, etc.), and the Qualtrics Bot detection software","I HATED IT. I ended up defending my thesis MUCH later than expected simply because of this nonsense. I had no institutional support which made it worse and I wasn't the only student this happened to. Universities need to invest in stronger software. There is only so much password protecting, validity checks, etc. I can do"
R_1ot1tJHaiUtaaVJ,A,1,None,Amazon MTurk,Odd responses that were completed in a very short time.,"We needed to collect additional data. Also, they would likely have compromised the statistics if not removed.",N/A,"Include multiple methods for detecting bots. If possible, avoid providing time compensation to every participant, and use things like raffles instead. Be careful when collecting data from populations with known bot issues, such as MTurk.","Set somewhat liberal a priori cutoffs (e.g., one attention check can be missed, short minimum amount of time needed to complete the survey). To me, this is consistent with data cleaning procedures that use p < .001 as a cutoff (e.g., univariate outliers of z-scores > 3.29).","Recommended steps for asking questions designed to detect even ""smart"" bots, without overwhelming or annoying participants.",N/A
R_24wVOWqGup3HlKE,A,1,"Robots have their own ideas in many areas, and their impact on fundamentals makes a difference in wages",FacebookÔºåGoogle,"Robots don't have human minds, and many studies don't fit together",Advancing my research,A random study of all aspects of robots,Don't give too much human emotion to the robot,I don't think so,"Huge databases, and real content","I don't want too many people to know about this study, because it's not complete"
R_21Fenq2DaKUiDaA,A,1,New security measures were implemented shortly after making the survey link available due to server farms and malicious users from outside the U.S. infiltrating the survey and entering bogus data. Data collection was paused and then resumed once the modification was approved by the IRB. Individuals who did not turn off VPN/VPS/proxy were not able to access or complete the survey. Individuals accessing the survey from an IP address that had already been used were not able to retake the survey. Individuals who failed the reCAPTCHA test were kicked out of the survey. Individuals who did not send a valid code generated by the survey were not able to receive compensation.,"Fetlife and Reddit mostly, after revising the survey.","Within an hour of making the survey link available on three subreddits and my twitter, the compensation survey completion number suspiciously reached the goal of 130 participants while the research project survey completion number lagged behind. After a closer inspection of the data collected from both surveys, I determined that bots/server farms/malicious people had infiltered the surveys and entered bogus data and emails.","Somewhat of a set back, but I learned a lot about setting up security protocols that I will use for all of my research.","Copying and pasting from the modified IRB: ...Would like to implement new security measures to dissuade bots/server farms and ensure quality data collection. These measures are peer-reviewed standard procedures (Kennedy et al., 2020; Winter et al., 2019) and/or recommended by Qualtrics Support. We would like to incorporate the following: Embedded Code: Each participant will be assigned a random embedded code when they enter the survey. Participants will be shown their unique code at the END of the survey and instructed to email the code to the PI, for it will be necessary to receive compensation. The participant must email the correct, valid code to receive payment. Participants will be clearly warned that they must enter a valid survey generated code to receive compensation. No identifiable data (other than for security questions, see below) will be requested in the research survey. Only PIs will have access to the codebook that lists valid codes that could be linked to emails/names. However, we will NOT identify or re-identify the data collected in the research survey. This method is only necessary for ensuring that real participants are compensated and bots/server farms are intercepted.   IP address cross-check: Because several real participants and ""fake participants""/bots completed the survey, we have a block that compares participants' IP address to the list of IP addresses collected from the compensation survey. In conjunction with the ballot box stuffing prevention setting, this will prevent previous participants from accessing the survey again. IP Hub or a similar IP location checking website will be used to cross-check IP addresses.  3. 4. 5. 6. VPS/VPN check: A warning at the beginning of the survey will tell participants that the checker will make sure they are not using a VPS, VPN, or proxy to hide their country. It will also tell participants to turn off blocking applications, VPSs, VPNs, and/or proxies- failure to do so may prevent completion of the survey.  reCAPTCHA: This test will ensure no robots can enter the survey. Failing this test will result in immediate closure of the survey. Participants will be warned and told to complete the reCAPTCHA test before continuing to the information sheet.  Embedded Data (ED) checks: After the information sheet, participants will complete an ED block. Participants will also complete another ED block at the end of the survey. The first ED block will ask participants to select from a drop down list their 1) birth month, 2) birth year, 3) current city of residence, 4) current state of residence, and 5) a question asking respondents to briefly describe what their task is in the study. The last ED block will ask participants to 1) type in a text box their age 2) select their city and state from a multiple choice list containing pre-selected small town names (both responses will be cross-checked with their answers in the first ED block), and 3) a question that provides a simple instruction (e.g., Describe your favorite way to spend 1 hour on a sunny day. Please write 1 sentence.) to prove human-ness. This identifying information will only be used for security purposes and NOT to identify participants.  Referral Website: One last method of security is to add reddit.com and fetlife.com as referral URLs. That is, individuals who click the link from those websites will be able to access the survey. But if clicked on through Facebook somehow, where the survey link is not posted, the individual will not be able to access the survey",NA,"I did the best that I could and if I had any suspicions, I validated those and stuck to my gut. I even commented on my posts that the survey had been attacked and some individuals reached out to confirm that they were real people.","Qualtrics support and those papers I mentioned previously were extremely helpful. Also, talking to someone more experienced that me really made a difference in my ability to integrate these security measures.",The steps I used were very successful in detecting bots and server farms! I recommend them to everyone.
R_2c7rAr9nFeEl3Mz,A,1,NA,"Facebook advertising, posting on walls, posting within groups",Multiple respondents within seconds with very similar information,It slowed down the screening process and forced us to create more hoops for real participants to jump through.,NA,Be skeptical of all survey data collected without direct participant contact.,NA,NA,NA
R_2Cx0WgM6CD8Z5Bi,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_31F544tQ2wRgdWD,A,1,"Initially, was not aware of bots so had to pay even the bot workers. Once realized it, needed to include more labor time to filter before isuing payment and still likely miss some",none. MTurk,Normative reporting was way off from the in-person sample,"limited sample, added additional labor for cleaning of data",NA,"See recommendations we've published in Chmielewski, M., & Kucker, S. C. (2020). An MTurk crisis? Shifts in data quality and the impact on study results. Social Psychological and Personality Science, 11(4), 464-473.",using multiple screening criteria and only drop if they fail more han 2,Use apps that capture timing/response,NA
R_0ANgJ16dp70qSLT,A,1,"In some cases, bots have been discovered during screening and have not gotten to the data collection phase. In other cases, we collected data from them and allowed them to enter in a raffle even if their data was poor if they finished the survey.",Mturk only,strange answers to free response questions,"This was a thesis project for an undergraduate student, so we just did not publish the study .",NA,We have gotten better at filtering bots out at the recruitment stage of the process. We have learned that we cannot post a public link to a study for people to directly sign up to participate without having a filter.,This is hard and I don't know the answers. We try to look for evidence and not remove without evidence.,More time,No
R_8hUB4lfLNuVkdqh,A,1,I spent about 50% of my original compensation budget on bots and then paying for services to combat bots.,Facebook - posted in groups for parents and teachers of preschoolers and asked them to complete a screener survey if interested,Fishy email addresses and formulaic responses,"Huge - affected my budget, made data collection take months longer than expected because I stopped posting in any public forums","Asked MTurk participants to fill out a screener survey that asked questions verifying their ""guaranteed"" qualifications on MTurk. (e.g. I paid to have the screener displayed only to parents, then asked in the screener if they were a parent.)",NA,"It is very possible that I removed real participants from my data pool, or accidentally did not invite genuine participants to participate in the study based on my analysis of their screener responses.",I don't know! I am really discouraged and now feel that I can only trust data from parents recruited through schools or studies administered in person. I had zero success using the paid featured on MTurk that are supposed to weed out bots and Prolific does not accommodate parent-child research.,"I denied hundreds of ""participants"" that I believe were bots on MTurk because their screener answers didn't match their supposed qualifications. Even if these were not bots, they were people that were not answering survey questions with any sort of consistency, and some of them were the same people with only slightly different email addresses over and over again. If I had not asked them to opt-in using email or asked them to re-verify their qualifications, I would not have known. I now feel that no survey data using MTurk should be considered valid."
R_3PKKNssYRXdmP2R,A,1,"We get exponentially more bots on paid surveys than non paid surveys. So it makes you think twice about payment for surveys. Even with prolific, it makes me think about how much I'm willing to pay, if it's worth it go through all the bot responses.","Facebook (parent groups), reddit, twitter",1000 responses in 5 minutes.,It made data cleaning really really hard.  WE're still struggling to get real data becuase it's so overwhelmed.,Video recording of data or audio recording of answers.,"Use all the tactics- qualitative answers requiring 2+ sentences, multiple attention checks, piloting to figure out minimum time, asking for bday multiple times. You have to do all of it.",I have no idea-- I don't think I'm doing it well.,"Some kind of automatic flagger. Undergraduate research assistant are so hesitant to make ""final"" calls and delete.",It's time sucking and also incredibly discouraging.  It has ruined the promise of online data collection.
R_2UgXTKGNXQgQKEv,A,1,Man-machine verification,Twitter,Conclusion bias,Outcome variance,Mail authentication,If-recognition robot,Collect more data,Mail authentication,no
R_30ozUqtrunH0yAY,A,1,"Initially, it impacted compensation quite a bit because I needed to rerun the study to collect more participants with more checks to weed out bots. At this point, I think I've gotten fairly good at ensuring higher quality data through quality checks and using a different platform",N/A,"in cleaning the data, the open-ended responses were nonsensical",lowered data quality,NA,"definitely recommend spending the extra money for ""high quality workers"" on cloudresearch. Also, adding in honeypot checks and short answers are good ways to detect bots.",I think so. I now pre-register my plans for exclusion of data and use that criteria to remove data whether they are from real people or not.,More money. Prolific seems to have less of this issue than Mturk/CloudResearch but Prolific also costs more to conduct the research,NA
R_2cvkXdFcHOoUkol,A,1,did not compensate bots,none for this study,name was suspicious and child had not participated at other stages of the longitudinal study,minimal but annoying/alarming,screened for suspicious names/email addresses,NA,NA,NA,NA
R_1KlOwK8HnkJQ3h0,A,1,No,facebook,I forgot,I forgot,NA,NA,NA,NA,NA
R_1mQqmz5FB5fdFGC,A,1,no,community,Robots work efficiently,Robots work efficiently,no,no,Robots work efficiently,Robots work efficiently,Robots work efficiently
R_AGt7o63ToSiWUmZ,A,1,"Payment of incentives to ineligible ""participants"".","Facebook, Instagram, Twitter",Dramatic increase in responding,Cost and labor to identify and remedy.,Requesting mailing address for incentive distribution,NA,"By trying to be very judicious, erring on the side of conservative exclusion.",Machine learning/AI,"Survey bots, fradulent responding, and falsification have been topics of study within the world of non-psychological-topic-based survey research for a while now. If you haven't already, please search journals such as Public Opinion Quarterly and the Journal of Survey Statistics and Methodology. Additionally, search survey research conferences such as AAPOR, IFD&TC, and FedCASIC. These large bodies of work are too often missed; creating parallel lines of research in adjacent fields."
R_2uHRK58CqdP9ysY,A,1,My experience with bots is that they have infiltrated our sign-up system for recruiting families to participate in online research. They have not actually participated in any studies because we weed them out.,Facebook paid ads,The names and contact information did not make sense,They are a huge annoyance for recruiting participants. It is sometimes hard to tell that a respondent is a bot.,NA,NA,This is not an issue - we simply don't schedule the bots.,It would be great if Google forms allowed some kind of captcha verification,NA
R_UKqq5aIHuaeyLG9,A,1,Made it much more difficult to decipher who should enter the drawing to win the gift cards...,N/A,"Inconsistent responses, fast completion times, missing data",Made data cleaning much more difficult; less confidence in data analysis,N/A,NA,Utilized advice from my advisor and other committee member (proficient in statistics),Better information on how to prevent and handle bots! (I now have many ideas to include in my next survey project...),It makes me SO angry...
R_xeXtjqFjqe9WOrL,A,1,I get hundreds of emails from participants claiming they are eligible.,Facebook n groups instagram stories/ posts,When I got to the interview the participants were not people with disabilities,They changed data collection,NA,NA,I do not ask to interview questionnaire responses that seem like bots,I have no idea. I thought about not offering monetary incentive,No
R_AurMB7TuePlyN5D,A,1,"we had a lengthy survey on MTurk that should have taken a minimum of 10 minutes--we encountered a sizeable number of respondents who took 7 minutes or less, many of whom showed evidence of response sets",none--relied on MTurk recruitment,suspicious data patterns,"took longer to collect data and did more through data cleaning to detect response sets of data that were collected ""too quickly"" (i.e., less than 10 minutes of a lengthy survey procedure)",no additional ones,look carefully fo responses sets in any data and always include attention check and repeat items for online surveys,"we compared demographics of suspect data with balance of data set and discerned no differences (similar age, education, etc)",ability to monitor and exclude IP-sourced responses; instructions on opening screens to provide special instructions that only someone reading the opening screen would know to do,"I was very disaapointed and surprised to find this in MTurk, as I would expect them to have developed an algorithm to better detect bots"
R_2YRDcsmOgisVJjB,A,1,"We became a bit more strict with who we'd give money to in order to minimize paying bots, but not much more strict.",NA,Analyses showed correlations between variables that simply made no sense.,"They ruined it, more or less. Reduced the usable n to a tiny amount + it was hard to fully trust even what we did have.","The primary thing we did that worked was to create questions at the beginning that had a TON of possible answers. We had four of them. The odds of the bots beating all four went down quite a bit, and then we didn't have to pay them because they were removed before participation. We'd then usually catch the rest of them at a later point with other techniques.","Take this seriously, it's a huge problem on MTurk. Use more exclusions and include a lot of checks -- bots can easily beat any one question, but you can beat them with a ton of them. Also, if your data don't look right, trust your nose -- if they look wrong, it's probably because they are.","I take a big-picture view of it. All data has error. That's a given. But if I replicate findings over multiple studies, then the odds that I'm getting something spurious goes down. I generally try to ensure that the data are clean enough that that can happen. On the flip side, I sometimes test data by applying a hyper-standard that excludes more than less (thus probably excluding real participants as well), just to verify that there isn't anything bad happening. If the methods yield different results (they haven't so far), then that's a problem.","I think it's absurd that we're dealing with this at this end at all. I think MTurk and other platforms should have more careful exclusion technology, or provide more help for it. I read all about it and then, after nothing helped, I basically invented my own methods which seemed to work. That's kind of pathetic, that the people care so little about researchers as that.","I haven't collected an MTurk study in about a year, so it's possible they've gotten better since then. I doubt it, though. Maybe other services have better safeguards in place, too. My estimates throughout are complicated because we've run so many studies where we've progressively upped our bot-detection mechanisms. The last studies we ran, we only get a handful through the initial screening and then we take them out on the back end."
R_3Jwus9hmBjx5SMc,A,1,"Now that we are aware of them, not much of an impact. We now have a clause in our ad and consent form that states that suspected bots will not be compensated.","Mechanical Turk, Facebook, Instagram, Craigslist","Open ended questions have been the most telling. Often in all CAPS, often clearly copy and pasted from some narrative on the internet, which doesn't pertain to the question being asked.","Wasted time, questioning the validity of the other data points in the study (those we did not clearly suspect as bots), frustration with the recruitment method",NA,"Always include open-ended questions! Also, I typically now avoid Mechanical Turk, since it has been the worst culprit.","We have taken a conservative approach, which makes us worry on the other end of the spectrum that some participants we believe are ""real"" are actually bots.",NA,NA
R_2ANHi8eFKQBTG67,A,1,"If I can clearly identify that the participant is a bot, I don't pay them. If it's unclear, I have to pay them. So I may be losing resources in the process.",NA,I identified really strange responses in my open-ended question that did not seem like they would've been produced by a human.,"They left weird answers to open-ended questions, provided impossible answers (e.g., being 18 but voting 60+ times), or copy-and-pasted the question back into other parts of the survey. I tried to not compensate when I could, especially if I followed research identifying bots, but I have very likely paid bots. I removed any suspicious data, however, from my dataset.",NA,"Absolutely clean your data and look for bots using research guidelines. Even if you think there aren't bots, do your due diligence.",I usually remove data that I think are highly likely to bots according to an a priori decision threshold. I have not done this (but could) - sensitivity analyses with and without those participants to see if it makes a difference.,"More research on bots and guidelines for what to do. More support from IRB about including language surrounding ""if you're a bot I won't pay you' - have gotten a lot of pushback on that from my IRB.","I have noticed a pattern of bots identifying as black, male, bi-sexual individuals. Not sure if others are noticing that."
R_1LLj7tHG93tuVFt,A,1,"None, we used Qualtrics Panel data, therefore all participants were compensated for their time in accordance with their agreement with Qualtrics.",None.,"Non-sensical responses, and inconsistent demographics responses.","Unable to attain desired sample size for estimated power due to funding limitations, i.e., wasted money.","Qualtrics recommended placing demographics questions prior to consent in order to filter out responses.  IRB approval was required, therefore could only implement in a limited fashion.","Be be prepared to lose up to 25% of data due to bots, and need to plan accordingly.","We are fairly conservative in removing suspicious bot responses, in order to reduce that risk.","More researching funding, and IRB approval if compensation based methods will be inovlved, e.g., only compensated human respondents.","Spoke to several vendors, and the problem seems to be getting worse.  Vendors also informally mentioned there is worry in the industry what programs such as ChatGPT will do to make this problem worse."
R_qyFKqH3crkO69tD,A,1,N/A,Facebook,"Unusual Email addressees, adults showing up for child studies",made identifying real participants challenging,Adding Additional Captcha throughout the survey.,NA,In some cases we don't know people are scammers until study participation when we see that the participant is not a child nor has a child.,Required video camera in zoom meetings.,NA
R_brSWktiPzmQyPgB,A,1,I don't know how to answer this. I was unable to gather as many participants because the bots were so expensive,None,Quality assurance measures. The bots entered nonsensical data,"I had to stop the study, redesign it, and recollect all the data",I added stages to the study and only invited select participants to the expensive second stage,Don't use mturk,I would rather remove one real participant than keep one bot,I'm not certain. I will no longer be using online platforms due to bots,"They completely detailed my dissertation process and I had to start data collection from scratch later. I did everything right as far as the best practices were concerned and still had major issues. I will never use mturk again, and I don't recommend it to anyone."
R_2VasND2ncguL4SL,A,1,probably minimal. We use methods for screening out bots prior to the compensation phase.,none,We assumed it would be impacted by bots right from the start. So we planned the methods in a way that would screen for them and remove them during the study process.,Just takes extra work to design the study in a way that would screen them out. And then extra work to carefully screen the data prior to data analyses.,screening tools prior to beginning the survey where bots may not know the eligibility criteria we are looking for.,Just take your time to design the study in a way that you can detect them. Then make sure to carefully screen the data so you are confident that the responses that you analyze are real.,Good question. We err on the side of caution and are okay removing some real participants to make sure we get all of the bots.,An agreed upon set of strategies that we can share that we followed with reviewers and they can be confident that we took all of the right steps. Some reviewers are set on bots being a problem that they won't even consider a study even though it included many steps to make sure the data were from real participants.,NA
R_1okh30mZvNC7yP9,A,1,Losing money to data that can't be used,NA,It was really obvious. Answers that didn't make sense for the study/were totally off topic,I have to overcollect by a lot to still get the Ns needed,NA,carefully review your data and remove bots as part of your normal cleaning procedures,"pre-register your criteria, make your criteria strict (which may leave in some bots), and ideally have multiple people independently review for bots",better systems in place on the online forums people most often use to collect data; we're paying a lot of money to these platforms to collect data and then are having huge problems because of their lack of action,NA
R_1QErZkxFyxlVIh5,A,1,Some bots may have received compensation due to my inability to determine whether they were bots or not. Many I was able to determine were invalid responses and did not provide compensation,"Reddit (public forums), Linkedin (private page but public post), Facebook (private page, but public post)","The vast number of responses in a short time, especially with incomplete data",I had to comb through 600+ responses by hand to determine which responses were valid and which were not,Including CAPTCHA at beginning of survey,NA,"If I was unsure whether or not a response was a bot or human, I kept the data. I only removed data that I was sure were bots based on the previous tactics discussed.","CAPTCHA, avoiding public posting, focus on snowballing","IRB had never heard of this issue when I contacted them, but I think it is more prevalent than people realize. Bots should be discussed in all research methods classes!"
R_3DedMXFsn63NBmM,A,1,"The compensation has not been impacted because the compensation is only through the subject pool, and the bots only from the social media recruitment (uncompensated)","Instagram, Twitter, Reddit",Qualtrics spam flag,fortunately minor,NA,"bots are an issue, use appropriate measures to detect, preregister your detection methods and how responses will be selected for removal","good question, not one I have a good answer for.","better tools for detection, better quality control in panels",NA
R_1isVs2J1kLMUan4,A,1,"Thankfully, I had a screening process set up prior to giving out compensation as I recruited from social media so had a little more control than if I had recruited from amazon mturk, etc","Facebook, Instagram, Reddit, Craigslist - paid advertising in Facebook and Instagram and public groups in Reddit and Craigslist","Through a mix of assessing IP addresses for duplications, assessing whether the IP address was associated with fraudulent bot activity, assessed qualitative data for coherence and repetitiveness, assessed duration of the survey, assessed whether they passed validity questions, etc","I had to screen out A LOT of bots in my data - in the previous study I think I accidently skipped it because I didn't fully understand it - about 98% of my data was bots. I went from about 10,000 respondents to 266 participants. It took an incredible amount of time each week to devote to screening the data. Thankfully it didn't cost any money because I screened prior to paying participants.",Use targeted recruiting on social media platforms,NA,"Tough question. I honestly assumed more bots were taking my survey especially since there was such clear evidence of bot activity (weird times, repetitiveness, IP addresses coming from across the world taking the survey repeatedly). I thought it was safer to be more conservative in removing bots.","I would want to have more knowledge about IP addresses, such as how to check on using whatsmyipaddress and other websites to understand how to assess whether an IP address is legit or not. More knowledge of programming (including Javascript to weed out bots). More expanded options for survey platforms (e.g., my school didn't pay for captcha use on qualtrics which would have been helpful)",NA
R_BY5e3wTMnOuOLNn,A,1,"Because of bot presence on mTurk/etc., I need to recruit more participants in anticipation of excluding many of them, and without grant funding, I must offer a lower per-participant payment than I would prefer.",mTurk,"In one study, approximately 60% of my responses came from the same 4 IP addresses. Rapid completion time. Vague short responses to open-ended questions (most commonly ""nice"" or ""good study"" in response to a question I include asking people to ""write a sentence or two describing their reactions to this study"").","Raises my overall expense, reduces per-participant compensation, reduces statistical power, makes the data cleaning process more lengthy and difficult, and forces me to write more in the manuscript about the data cleaning process (a pain when many journals have word limits).",N/A,Check IP addresses if available. Plan data cleaning criteria ahead of time. Anticipate about a third of mTurk responses will be bots.,"I employ a two-strike system; if a participants' data includes two or more reasons to suspect it's bad data, I remove them. This allows for a little leniency in basic human error, but is still strict enough to catch bad actors/bots. The majority of bot checks I employ also double as attention checks, so I feel reasonably confident that the data I exclude from analysis is either non-human or an inattentive human, both of which are bad for data integrity.","I'd love a way to catch bots *before* I pay them for doing my study. Anonymous surveys means I can't ethically link the participant profile to their data, so I can't ban bot profiles from taking future surveys and I end up having to pay the bots regardless with my current methods.",They suck. I hate them.
R_1LXVb7CF274V5Yo,A,1,In my study I was able to easily filter out bots because I required participants to film themselves so they were deleted before compensation was given to participants.,"Posting on personal Facebook and Twitter pages, posting in Facebook groups, posting in relevant Reddit forums","I got 100s of participants overnight, and none of them had video recordings which were a required part of the online survey","I had to waste hours deleting every bot (there were over 1000 total), and because of the survey host I used (Phonic.ai) I had to delete each bot 1 by 1 since I was only allotted a certain number of participant credits on their site. Not deleting the bots right away would limit my ability to download the real data.",N/A,NA,"While it isn't perfect, I just deleted every participant without a video/audio recording. There is a chance a few real participants were mixed in but it would have been insanely time consuming to check in other ways as well so I accepted the risk of losing real data.","Effective bot screening built into the survey site, such as captcha.","Other than just bots, I also had participants who were real people but obviously not qualified for my study and lied about their demographic information. My study was meant to be for children aged 7-9 and I had several participants with fully grown men in the videos taking the survey anyways, likely for the monetary compensation."
R_26rgkFhDHW0crqF,A,1,the only studies we've gotten bots are ones with compensation (both guaranteed compensation and an opportunity to enter for a raffle). I had to then modify my IRB to collect IP addresses and do a verification follow up. it took me days to comb through the data.,facebook. twitter and instagram,I all of a sudden had thousands of responses overnight,I had to cancel the project and only had an n of 79 to present with. I was rejected from a big conference likely due to this small sample size. I spent hours I didnt have cleaning the data and it was a HUGE blow to my passion to the project.,NA,Do NOT post your study on social media.,"honestly I probably removed real humans- I was so frantic at this sudden derailment that I probably overcompensated, and there isn't a lot of guidance out there!",Qualtrics apparently has a bot check where it assigns a number based on bot likelihood in the data. My university doesn't have this function and i'm lobbying for them to include it. I also will never post on public social media again.,I'm nervous that you posted this on twitter and are going to have tons of bots but I will practice radical acceptance and let that go :)
R_5mTaBGWWvngCX7P,A,1,Afraid I‚Äôll send it to a bot email - compensation is on a lottery,Twitter & Reddit,Way too many responses,"Additional stress, messes up data",NA,NA,NA,NA,NA
R_3nuEEhiAkoVU5cg,A,1,"Some bots that I was unable to notice probably received compensation for study completion, which reduces chances human participants receive it.",Amazon's MTurk,"I had a section in which I asked participants to write about how it felt to be financially constrained. I could tell bots had responded as the answers made no sense, i.e., defined the words ""financial"".","Definitely made data collection take longer as I had to screen every response, then reopen the survey, and check again until my sample size was met. Also a lot of frustration.",N/A,NA,"This was definitely something that I and my PI spoke about a lot, and I do not think we came to a clear answer.",Recaptcha,No
R_3JEmcHHlezNAlLH,A,1,NA,NA,NA,NA,NA,NA,NA,NA,NA
R_3kFibC0xdSPOnOf,A,1,"early on, we may have paid bot accounts, which reduced money we could pay to actual participants","Facebook, Reddit","we included multiple items to try and detect bots (that may also detect random responding) such as ""honeypot"" questions (those that are visible only to non-human, automated responders, open-ended questions to look for nonsensical answers, multiple questions addressing the same topic to assess for response consistency","very little, they were removed during early data cleaning before paying participants",n/a,use multiple types of questions to screen for bots,participants would have to answer multiple types of questions in a manner similar to bots to be removed,unsure,n/a
R_3CC0sP6hPbnNZ2o,A,1,None. We have to meet fair market value compensation requirements set by our clients who are pharmaceutical clients,"Paid ads on Facebook, instagram",Qualtrics suspected bot rating,"Not much. We use strict security settings in qualtrics to prevent bot activity. We require US ip addresses to even open the survey, we adhere to qualtrics bot ratings, and we remove similarly patterned emails that come in right after another. Additionally, we only pay participants who use a bank verified account where their survey name matches their bank account or an email outreach. If it doesn‚Äôt, we discard the response.",Payment methods that require verification of identity,Multi-layered processes. Payment to each respondent forces verification of identity. However I work in the private sector where the rules for respondent compensation are different,"Sticking to our predetermined definitions of bot behavior. If soemthing feels weird but passes our parameters, well guess what sometimes humans do weird stuff!",I‚Äôm set. Maybe an AI tool that could go above and beyond what qualtrics and human evaluation offer,"Know that it will happen, and that you have a clear plan for when it does."
R_TtjslWr8CV9emQh,A,1,Not at all AFAIK; I identified and removed them,Facebook,Too many responses and responses had similar emails consisting of NameNameNum eg JoshuaPitanski657@hotmail.com,None; we did qual interviews and everyone was a person we wanted to recruit in the end,Unusual emails (we collected emails to send follow ups for qual interview scheduling),make sure you have some kind of system in place to detect; the exact needs might vary study by study,Sens/Spec; I'd rather remove a few unusual real ppl than keep in bots,"effective ""not a robot"" Qs",NA
R_1ffI3nzvDHfagSS,A,1,Minimally. I only compensate after screening for bots/careless responding and (as listed in the consent) deny bots or bot-like responding,None. Primarily MTurk is where I encountered bots.,"The responses given, qualitatively and quantitatively (including manipulation checks), were nonsense or didn‚Äôt make sense in tandem. We also included specific measures to detect bots which were piloted and validated in other studies.",Took much much longer to collect all the data and added a lot of extra work to screen and clean the data. Over doubled the expected time to complete one study.,Asking for age as an opened ended response worked weirdly well.,"Add as many checks without compromising the survey length or effort required. Also‚Ä¶clean, clean, clean your data.","We created a cut off for certain responses and used multiple eaters to assess for bots. In the end, I‚Äôd rather throw out lazy responding with bots as well",Make MTurk better.,No
R_33du1dE8fbZqbW6,A,1,"In the first study, I was able to detect them easily. However, in the second study, we had to pay mystery people","public and private groups. Often focused on LGBT identity. However, there were some general groups too",When the data did not align or make sense. I had a two step screening process.,"In the first study, not much. I was able to weed them out. On the second study, we had a harder time. It was a interview and we had people show to a Zoom with no camera on and no voice. They were supposed to be older women, and we could not verify this.",NA,"Use multiple levels of recruitment. Lots of attention checks of different types. If it seems to good to be true, it likely is!",The two step process helped. So did the attention checks. There were some difficult decisions to make with 1/2 people.,Automatic attention checks. Or timing allotments.,NA
R_3IaWK7oEeodU0dH,A,1,It was a raffle survey so I worked to remove the bots and then emailed winners of the raffle directly to confirm they were real people before sending the gift card.,"Facebook, Twitter, Outreach to Community-Serving Organizations; we can tell the bots definitely came from Twitter",Sudden increase in responses with nonsensical answers to qualitative questions and height/weight numbers,Required heavy data cleaning and delayed the project by several months,"Used a changing qualitative question, such as one day having it ask ""What is your favorite color"" and another day ""What is your favorite ice cream flavor"" and removing invalid responses to those questions.",NA,I really tried to review the results for things that I was nearly certain would be an invalid response from a human and tended to lean towards deletion in cases of significant ambiguity.,"Best practices resources for preventing bots, sample items for catching bots, recruitment strategies that avoid bots","I forgot on the last page to mention that I also looked at the emails people provided for compensation and if they were a nonsense combinations of letters and numbers and/or from a suspicious domain, that also was a red flag."
R_1pANFmiLm0AspeQ,A,1,"My compensation survey is always separate, so some bots find both but it does delay/fix some of them.","Reddit, specifically subreddits. They did not appreciate it me talking about whiteness.",After a huge influx of nasty rhetoric + a large number of random responses (think 100+ in one day when I'd had a steady 20 per day).,Ballooned my sample size and made me delete ~200 responses,Using the reCAPTCHA feature on Qualtrics to ID certain types of mouse movement and the Google reCPATCHA score.,NA,I required that respondents miss multiple forms of verification in order to be deleted.,"Built-ins in Qualtrics really help, as well as ways to better detect qualitative data mistakes.",NA
R_9FEPl3Tv21WCmo9,A,1,Not sure what is meant by this,I used Facebook in public and private groups.,I had a large influx of 100 respondents in 1 night.,"It was a negative impact, I had to parse through the data to try to determine what was a real entry and what was a bot.",N/a,NA,"A lot of the bot responses had similar reasons they were suspicious so it took a while to review responses, but easy to deduce who was a bot.",Literature and trainings on how to preemptively deal with bots. Survey plug-in that automatically detects that survey is under attack by bots,N/a
R_2Qy15IqQMFweOkY,A,1,"I spend a lot of time trying to sort out bots, but I'm sure I get it wrong sometimes, resulting either in bots being paid or real people not being paid, both of which are major problems when you are using small grants or trying to create relationships with the population you're studying","I didn't post to social media, but participants did post to FB groups and that's when I was attacked by bots","Big flood of unexpected responses with emails that looked fake, IP addresses from all over","Made data collection much more strenuous, dozens of hours were spend evaluating responses and revising procedure.",Asking participants to not post study info on social media,"Honestly, make it as hard as possible for bots right out of the gate because it's so much harder to address once you've started recruitment. Researcher also need much more  sophisticated data collection software to prevent bots, because bots are getting much more difficult to quickly identify. Institutions should be allocating funding for bot detection software, because this is a major threat to research integrity.",Tried to create multi-step decision making processes to evaluate if it's a real person. Included qual responses so participants have a chance to demonstrate that they're throughtfully responding,BETTER SOFTWARE! BOT DETECTION! FUNDING!,"I said 90% of my data had come from bots, but by the end I believe we got it to 0%. It is possible to do research with integrity, but it taxes the researchers much more, which is a problem when we are grad students with no time, resources, or power in obtaining additional software."
R_2zBzcwlefiz87q9,A,1,"Immensely on MTurk. I had a study on a very nuanced population (non-English speaking user of a substance that received a healthcare intervention in the past 6 months at a healthcare facility in the United States). In addition to checking IP addresses, I use 10 different attention checks of varying types (direct queries, logical statements, open response) and I'm able to say about 4% of all respondents actually inclusion criteria. Hours and hours of data cleaning for this and then of course, to respond back to Turkers about why they were rejected. A good portion of this has to be bots and not individual humans. It's maddening.",N/A,"Immense respondents from the same IP address, and after getting scanned out, then reenetering different patterns of responses to gain acccess to formal survey,",Slowed down data collection immensely. Research fatigue. Detailing a PRISMA-stlye document to show why folks were excluded to show the verification process to better ensure that our sample actually is who they say they are.,NA,Assess in multiple ways. Detail these in your studies for better transparency about how intrusive bots are regarding recruitment. Build extra time into your study timeline to account for this.,"Again, just trying to be as transparent as possible about the removal process in the research documents and having them included when we submit the manuscript.","A GA that follows my detailed protocol. Additionally, more information from reserachers in the field, and also other fields (e.g., computer science) about how to navigate this with best practices. It'd be great if MTurk actually did some things to better counter this.",They suck. Thank you for doing this research. I'm curious as to how you will leverage this research for advocacy.
R_1lgBHMr2IvqLmcL,A,1,We have to approve compensation through Rybbon; made it more difficult to distinguish who to compensate (real people) vs not,"Facebook, Reddit for parents, etc,","Over 1000 responses in an hour, huge increase of rate of responding","slowed data collection down like crazy, revised recruitment methods, enhanced cleaning procedures‚Äî will have concerns/limitations of generalizability in the future","Added password to survey (didn‚Äôt help), added new password where participant's had to reach out to us directly",NA,A lot of participants took the survey within the same hour after it was posted on Facebook (+800 responses); those were all cleaned out after looking at submission times and IP addresses (repeats or very close together in time). We are doing a second wave of data collection through prolific where participants are pre screened,needs to be taught in methods courses for people doing survey research,I am convinced they highlighted or copied some information from the consent form or used chatgpt/ai to generate responses. I also noticed a delay with in submissions (same answers submitted at different times to pass the time check and look more variable)
R_2TAT4IEVnPZlMnY,A,1,They have made it hard to determine who should receive compensation.,Instagram,Too many responses in too short of time. Data did not make sense.,They made it hard to trust the data and that data had to be thrown out.,NA,NA,We likely removed real data in our attempt to be overly cautious.,I am not sure. I would like suggestions. I am skeptical about recruiting via social media when there is compensation involved. I may only use social media if I will be consenting participants via phone.,NA
R_V3gs4L634pt5I3f,A,1,"Luckily, we were able to identify the bots and did not send compensation to the associated email addresses.",Public posts on public Facebook and Instagram pages associated with our targeted demographic.,"In my study, interested participants were to email the research time to request a Qualtrics link (SONA participants were direct straight to Qualtrics and we could identify who was a SONA participant because their SONA ID number was embedded in the survey). We had setting on that participants could only take the survey once. It was at the very beginning of the semester and I had an email request for the Qualtrics link, to which I responded by sending the link. Later that day, I noticed that we had around 20 new participants. Since none of the new participants had a SONA ID embedded in the survey, this meant that all new participants must be email participants. However, since it was the beginning of the semester, I knew that I had not sent out 20 Qualtrics links, just a handful at that point, so it was not possible for us to have that many new participants. I immediately paused the survey and contacted the Qualtrics liason within our university's IT department who was able to confirm that the new respondents were bots.","We had to immediately take down the study, report the incident to the IRB, and then submit a modification to the IRB to help prevent bots from accessing the study. We were down for almost 2 months.","Since our study was examining sexual experiences of college men, participants could only use their university affiliated email address to sign up for the study.",NA,Implementing the university affiliated emails to sign up helped ensure that there was a real person associated with the email address as only the university could create these email addresses.,"Utilizing options within the software to help detect bots was helpful, but I didn't even know they existed until after this became a problem for our study.",Another one of my lab's studies has an in-person or virtual baseline component. Even that study has had problem with bots where they sign up for the virtual option then refuse to turn on the camera or use the audio to respond to questions. We have had to make adjustments to the protocol to address these issues.
R_sBB4sTC0sLcIdIB,A,1,"In our most recent study, we have had to send e-gift cards to physical addresses, as opposed to email addresses. This helped ensure the participants were not fraudulent responses.",We used paid advertisements on Facebook platforms.,Really quick recruitment in a short period of time (i.e. 20 responses in 5 minutes),"They impacted the procedures, but not the data.",We require participants to receive compensation at a mailing address located in the state of New Mexico. One of our inclusion criteria is that participants must be located in NM.,"Research best practices for avoiding fraudulent responses, create a fraud prevention plan, do a soft launch of the study, have staff specifically dedicated to monitoring each individual responses and enact the plan, be prepared to adapt.","We use a screening survey to help filter out inauthentic responses. Then, provide an individualized link to the full study. We remove any suspicious responses from the data set. We anticipate the removal of responses in our power calculations. Better safe than sorry.",A 20-hour GA/staff position to monitor incoming surveys.,"There are a number of resources out there that document current, best practices."
R_1CvXnDZ61ImtXtG,A,1,I continue to offer compensation but include a variety of screening measures to identify bots.,"Facebook, LinkedIn, Instagram","Many survey replies came in rapid succession when my study was posted on Facebook. The participant names were suspicious, the email addresses seemed phony (they did not relate to the provided name at all and rather were strings of random letters and numbers), and mailing addresses were either not provided as requested or provided but not real.","I implemented a range of new screening methods including attention check questions, data verification questions, and a requirement that gift cards be mailed via postal mail.",NA,NA,"I have erred on the side of removing participants who have crossed a minimal threshold of fraud suspicion‚Äî only one inconsistency/incongruity is permitted (eg, an attention check question incorrectly answered). I would rather continue recruiting new participants who are certain to be real than include suspicious data.",Recommendations about how to phrase online study advertisements so as to minimize their attractiveness to bots.,NA
R_21cZ802k7VOexZd,A,1,NA,NA,Saw a slew of responses coming in,Added time needed to clean/remove bots,NA,NA,NA,NA,NA
R_1kYMtSJpkSLRW5K,A,1,NA,N/A,The pattern of responses,Monumental because I had to cancel both studies (which were re-purposed to be conducted online because of COVID).,"In my recent studies, I have adopted all of the aforementioned selected methods to really make sure they don‚Äôt seep through","Don‚Äôt use MTurk! Prolific is more efficient and in my experience, the checks they have in place have given me peace of mind. ,",Not worth the risk ‚Äî remove the data and collect more data in additional waves.,NA,They‚Äôre the worst!!!
R_6zleWEn5cZKhI2t,A,1,"Our ""bots"" are actual families but they use fake emails and locations so they can try and participate multiple times, if they get through and we don't catch them we have to compensate them but are unable to use their data.",Facebook; Paying for Ads through Meta Business from our Lab Facebook Page,Got over 60 submissions in one day when we typically only get a few. When checking the IP Addresses they did not match the location we are recruiting from and where they said they were located,Paused on recruitment for over a month,NA,NA,"We have tried to just not include the bot families in general. If we think from their interest form that they are a bot, we do not contact them to schedule.",Database of developmental researchers to post emails number numbers and family info they think are fake families and then be able to search the families on those databases. Create a new system that helps detect when a person's IP address does not match their actual location,Bots in developmental online studies with families are actually REAL people who fill out our forms and try to participate. 1 case the family was just not from our area and spoofed their location to participate in another case a man came onto the study call and said he did not have a child available to participate so we could not test him. Sometimes real people/adults sign on and act like kids and we can't require them to turn on their cameras so we cannot be sure they are real or not.
R_DufdzFuSwlwiYMx,A,1,None,Twitter,Too many recruited participants in a short period of time,I changed my recruitment so that students would only receive compensation if they provided their school emails and the bots did not know how to do that. They only provided gmail accounts,NA,Be ready for bots if you use social media for recruitment. Make sure you have checks before your compensation goes out and check the number of respondent often to catch times of day where too many respondents are taking the survey that is not realistic,"I changed the requirement to be school email so regardless of they were human or not, they did not follow the directions which gave me balance for that risk",N/a,No
R_0NgP40uTcusi46d,A,1,"We created a separate form that has to be filled out after filling out the survey to receive compensation. Typically, bots are not able to reach  that form.",Facebook boosted posts and public and private groups,The names listed on the recruitment sheet were really weird. It was like someone trying to think of a name that seemed human‚Ä¶but was not quite human. And then frequently the name did not match the email provided.,"No real impact for this study in that we caught the bots at an early stage and did not reach out to them to participate.  In instances where we were not sure about not status, when we reached out to the provided email no one responded.","None. But just as a side note in case it is helpful for you, early childhood is from about infancy to about age 7 or 8. Middle childhood is about 7 or 8-11. In case you discuss that in your paper!",I would recommend learning what bots look like and taking steps to protect surveys with a two step method where taking the survey and receiving compensation are separate.,Just trying our best to verify that bots are bots and not real people,NA,NA
R_2tzogDVUkk3aUlM,A,1,"somewhat, since we have very strict screening process",Facebook,When I saw unreasonable responses with fast-pace enrollment,"Glad I detected that at the beginning of my study, so I waste some of my research funds and start to build more strict screening process. It is okay for my study since then.",NA,knowing it exist ahead of time and plan the extra component in your study design to address this issue,"I would be relatively conservative, only remove very ""unreal"" responses, given my strict screening process that likely resulted in more real participants than bots","research assistant to do phone call, meta-data and demographics comparison, IP address and other information double confirm",N/A
R_2fDHkVdQQVbM9hQ,A,1,"Compensation was not automatic, so we had to filter the data and withdraw compensation for bots.",Facebook,The night of study publication,Hundreds of fake survey respondents,reCAPTCHA,NA,"This was our major concern, but the participants removed had several indications of being fake respondents.",To become more familiar with cross-validation strategies. Maybe have some form of confirming identity before compensation.,NA
R_1l6oylilDeGsXuK,A,1,"We have to ask more questions to prove people are not bots, which makes the surveys longer, which means we need to compensate people more.",NA,Malfeasant response patterns but passing attention checks,Null data,NA,Include several ways to screen them out because some bots can pass some checks but not others.,We pre-register our checks and acknowledge this sacrifice,"Continue talking to colleagues about the checks they use. The bots are always learning and getting smarter, so we need to too.",NA
R_1QL6LORUQxixz5u,A,1,they haven't- they are typically easy to figure out and weed out of the process.,Regional-specific Facebook public and private pages,It had 100's of responses overnight,Mostly just time-consuming to remove and filter them,"Searched for different versions of same name used for both parent and child (e.g. John Adams, Adam John)","If you're planning to recruit from social media, install a regional geocoding filter on the survey first to ensure respondents are actually from areas you are targeting.","Sending secondary follow-up emails for second in-person phase of study, and if no response, then delete.",Better detection algorithims,Not sure
R_1JXGZydbAf8jKv6,A,1,"We have still had to pay participants through MTurk due to ambiguity in IRB implications of bot researchers. Since the bots were tied to accounts that could accept the money and we were initially not clear enough in our informed consent that certain criteria indicating a bot was used would result in rejecting data, we had to pay bot participants and exclude the data.",n/a,The time taken to complete the study was unusually fast and/or scores on RECAPTCHA were low.,"We had to take extra time and effort to comb through the data to try and detect and remove them, then run more participants to fill their spots.",RECAPTCHA scores,Be aware that they exist and proactively plan for ways to prevent a high volume of them in your study/have multiple forms of attention checks in place to detect and remove them. Also be explicit in informed consent language/IRB applications so that your actions for removing/rejecting bot data fall under appropriate ethical guidelines.,"Recognizing that it is a risk, but one that is offset by the potential harm bots could do to your findings. I find it less harmful to accidentally remove 1 real person's data if we still have a robust and representative enough sample if it means ensuring that 5 probably bots are also removed.",More sensitive RECAPTCHA/ways that a platform can automatically detect unusual patterns and alert to cases to review.,It would be beneficial for platforms like Qualtrics to have more robust built-in detectors.
R_2QMrzmJa9VJw6ey,A,1,"None. Bots were not able to access codes, gift cards, etc.","Facebook parenting & ""mommy"" groups (public and private)",Study participation sharply increased over a short period of time not explainable by other factors (e.g. method of recruitment),Slowed data collection; made for a painful data cleaning process,Participants screened via telephone before being sent a personal link (rather than a single link for all participants); paricipants had to provide email in a separate survey to receive compensation.,NA,NA,NA,NA
R_2zjMOYMcYF19fJ6,A,1,It makes it extremely difficult to determine who should and should not rightly be compensated.,"Facebook, Twitter, Instagram; both private and public groups",The rapid number of responses,We had to discontinue the study (for now).,NA,I don't know yet. We saw a huge increase in bot responses at the end of last year (2022). We are still trying to figure out ways to move forward.,"It's a really uncomfortable process. We looked for solutions the eliminate bots where we had more certainty and/or could be systematic in our approach. Still, it was an imperfect process and I am not at all confident in the data integrity for this reason.",Anything and everything. It has been difficult to find anything comprehensive--it's mostly pieces here and there from a variety of sources.,"We honestly couldn't tell in our survey if some of our ""respondents"" were bots (as defined at the beginning of this survey) or how many were real people but pretending to be a respondent rather than answering truthfully."
R_1KjpT2wJibsulqQ,A,1,Participants cannot be paid until successfully completing a screener survey.,Twitter / Reddit posts with relevant groups / hashtags,"Lots of participants (like, 400) with weird alphanumeric email addresses completed the screener survey overnight. I woke up with hundreds of new (fake) participants to screen",I stopped using social media for recruitment,NA,NA,"I work with a highly-constrained set of participant characteristics. If there's reasonable a chance it could be a real human, I opt to keep them but flag the participant in my notes. I have not yet reached data analysis, so I don't know what I'll do with those notes.",Better ways to reach participants from certain groups,A lot of bots are both blind and fluent in American Sign Language.
R_DMH8v7ZPikhijtL,A,1,N/A,"Facebook, YouTube","Hundreds of response emails with very similar names, email addresses, etc within seconds of each other.","final sample size greatly decreased, revision of recruitment strategies, revision of protocol (must now confirm eligibility, demographics, and inform that payments are linked to SSN for tax purposes prior to enrolling rather than after completion)",comparing email addresses exp. Johnsmith vs johnsmithh or comparing names and email addresses. There would be a response received from a 21 year old Jane Doe at 2:30 and a 21 year old Jane Doee at 2:31,NA,"more involved prescreening, individual survey links",Tech that keeps record of IP addresses etc,"After calling to confirm demographic information and informing that SSN would need to be confirmed prior to payment for tax purposes, many people stopped responding."
R_1MPWp2MIGz1V2yb,A,1,the study that offered $15 compensation to everyone was riddled with bots but when i've offered a lottery with giftcards it is much less,"facebook, twitter -- just a post that could be shared",i got 3000 responses in a very small amount of time,"it took so many hours to sort through all the data, decide who to pay, decide which data to keep. The amount of labor it took was incredible and i'm still not 100% sure that all the data is valid. Further, it cost a lot of money to pay people who didnt, or potentially didn't, validly complete the study.",NA,NA,"I think its possible that some did get removed, as its possible that some bots stayed in the data, but using several different strategies together we think we kept most of the human data.",a more systematic way of filtering them out or keeping them from getting in. Standard procedures for evaluating data for bots.,thank you for asking!
R_2e24IPQ1wEkd7Iv,A,1,Not by much,"Facebook, Twitter, Reddit",Similar responses back to back and sometimes from the same ip address,Not much. I deleted the data because it wasn't a lot and just collected additional data to meet my participant quota.,NA,Always be careful and do some preliminary data cleaning so that you can detect bots early on and your data collection does not get delayed.,NA,NA,NA
R_1jdignlp9lMEExw,A,1,"I'm not sure what this question is asking. My compensation methods have not changed as a result of bots, but I'm sure some realistic-answering bots have made it through the attention checks and manual review and have been compensated accordingly.",None,"I have a flag variable embedded in Qualtrics that flags any participant who fails at least one attention check in the study (typically my studies have at least 2 attention check questions, one harder than the other). I also check answers to open-end questions; usually bots will have nonsense answers.",None; I recruited a number of participants that was slightly greater than I needed so I would have sufficient sample size of the data after removing the suspected bots. It's possible that a few bots that managed to pass the attention checks made it through to the final dataset and contributed some nonsense data but I'm confident that nearly all of the data I used in my final dataset is from real participants.,Adding a captcha verification to the survey,Bots are inevitable so just make sure you have measures in place to identify them for removal from the dataset or rejection from the data sourcing platform. Always recruit more participants than you expect to analyze.,"Using attention checks also weeds out real participants who are not paying attention to the survey. I don't think it's a bad thing to remove inattentive participants, even if they are real, since they would be doing the same things as bots to the study data (i.e., adding more noise and potentially skewing the results). You probably shouldn't include too many attention checks in the survey or else real, attentive participants may get annoyed, but I compensate everyone who takes a survey so it's not like they're doing the survey without benefit (unlike if compensation is raffled off or other methods where not everyone gets compensated).","Certain platforms are better about reducing bots in their panels than others (e.g., Prolific seems to have fewer bots than MTurk).",I'm a bit concerned that ChatGPT will make it easier for bots to seem more realistic (at least in open-ended responses). It's uncharted territory but definitely poses a risk for future research.
R_2aE4qgD0pSBj5Ud,A,1,I have had to pay bots for participating in my research! But usually only 80 cents to 2 dollars,"I have collected data from Facebook, but only once. Usually I just post an ad on Mturk",They wrote strange things in the comment and ‚Äúwhat is this study about‚Äù section at the end of the survey,"Not much as far as I can tell, except impacting my sample size and wasting money (which is very frustrating!)",My labmate created a function in R to detect bots,"Implement attention checks, and pay for the version of mturk that uses higher quality populations","I stick with the rules I set for removing bots‚Ä¶.so failed attention checks, and also if someone responded to a free response item in a way that completely was irrelevant to the question‚Ä¶obviously there is some subjectivity in this method!","Honestly, I would just use higher quality data sourcing companies",They are a menace
R_2ZQ3XCWpjrNtfbI,A,1,Haven't raised compensation as much as I might have otherwise,N/A,"nonsensical responses, speeding",minimal; we detected most of them early enough,CloudResearch builtin features,Use multiple types of checks and it's worth it if you can shell out for services like CloudResearch,Largely by trying to make the bot failures need to be fairly egregious and nonsensical,"Built-in features, pre-built questions to detect likely bots, etc.",I'm not only concerned about bots but also false responders more generally and it's not always the same things that work to detect both
R_RntfAFwsH49AKWd,A,1,I occasionally accidentally pay out participant compensation to a bot.,NA,Nonsensical responses in open-bix questions.,"It was annoying and required me to do more work (data cleaning and ensuring that I remove all bots). It also led to a reduction in trust on my part in the quality of the data I was getting. As a result, I tend to increase my sample sizes now because I'm worried about noisy data. This also means that the costs to run studies is higher and sometimes deters me from running studies. I know I said that about 1% of responses are bots but that's only what I catch. I have no idea how many not responses I'm missing.",NA,"Have open-ended questions, check your data carefully, consider increasing your sample size to account for extra-noisy and bot-infused data.","I try to set clear rules that I follow for removing responses. These include excluding for failed attention checks, nonsensical responses to open-box questions, and other measures. It is possible and likely that I'm also removing humans with these measures but I do want my data to meet a certain standard of quality.",I wish Prolific and Qualtrics did more to identify and block bots.,NA
R_11WUqYNG7UnAnrZ,A,1,haven't,Facebook ads,a lot of participants signed up super fast with weird emails,had to develop new recruitment strategies,captchas,NA,we tried to look at IP addresses and kind of had to guess with some emails,bot checker,no
R_1PSI84gELSxpH9y,A,1,"If we detect bots in a reasonable amount of time, we do not compensate them. However, there have been many times where I have lost funds to bots.",N/A,They wrote nonsense to a free response attention check.,"I try my best to detect and remove them. If I did a good job, then the main impact they had was impacting my time, and funds, and mental state. If I was not able to remove all of them, then they likely impacted my results by either inflating or diluting my reported effect sizes.",NA,"To use multiple methods of bot detection, one method is not sufficient or rigorous enough. For now, I believe creative free response questions (e.g., write a sentence you believe no one has ever written before), does a pretty good job at detecting some proportion of bots.","I think about the properties of my study, the likelihood that bots would have a strong impact on my results, and the higher the risks are the more stringent my exclusion criteria are.","Papers or ""how to"" articles on what checks to use, when, and how. Or some kinds of technological programs that would help me screen my data post-hoc for people who may be bots.",NA
R_28RHSJbYUPjdwwE,A,1,not at all,CloudWorks,Open ended answers didn't make sense,They changed the p-value,NA,NA,The open responses were clearly incoherent,I'm not sure,NA
R_3W9XFQSRPeHFRfz,A,1,NA,Amazon MTurk,When scrolling through the data before data cleaning,"We haven't done much with the data yet, but they will probably just be excluded from analyses",NA,NA,N/A,"Best practices for avoiding bots in the first place, as well as best practices for eliminating them from analyses",NA
R_32RB9Fo93thTFK8,A,1,"I prepared for my study knowing bots may be present, and used a captcha and questions that would catch them, so to my knowledge none of them received compensation.",NA,"So even after trying to prevent bots, I could see blank responses from a number of bots that had failed the captcha (about 10% of my survey). Thankfully the bots did not receive compensation.","Really, none due to the captcha, unless some made it through without my knowledge.",NA,NA,"Since I was using MTurk for this study, I could communicate with participants and explain why they had been denied compensation if I thought they were actually a bot. I tended to lean towards changing them to be a real participant if they responded and could tell me what the survey was about, or explain why they answered the way that they did or were confused about the question.","I truly hope I do not have to conduct an online survey again... (I've since shifted to using genomic data) I would probably want participants to contact me somehow to verify that it's them taking the survey. I don't know how this would work in a secure fashion, though.",NA
R_2zeIeAtldUO9Ob0,A,1,"We usually try to weed out bots if we can, but sometimes cant find all of them. It takes up a lot of time to try to find out who they are",n/a,Through cleaning the data I realized some of the participants weren't making sense in their free responses,"The sample size was smaller than expected, but still fine since we usually over-recruit. The biggest inconvenience is the time it takes to find and take out bots",NA,It is best to over-recruit and set strict standards on CloudResearch,"I believe that I have struck a balance -- I, along with my lab, have developed a comprehensive cleaning technique. We also do robustness checks on the data - we analyze all participants AND those we believe are not bots. If the results are different, we report that in our papers","As of now, my system seems to work fine it just takes a while",no!
R_3kBpW2LJRjK1bOL,A,1,"Anything flagged as potentially a bot is rejected, so they aren‚Äôt compensated. This takes time, which means actual humans have their compensation delayed to allow me time to look for bots.",NA,The first study years ago had a bunch of participants failing attention checks. The most recent one has tons of participants from the same geolocation but they can pass the checks IP restrictions.,"It took much longer to finish data collection because all HITs were taken by bots, which then needed to be checked and rejected before an actual human had a chance to do that HIT.","It might be in meta data, but geolocation data was most reliable.",Don‚Äôt bother with online studies unless you have tons of time to remove bots.,"In the rejection, I include a message saying to contact an email if it‚Äôs believed to be an error. No one did.","Extra people to remove bots, methods that can be quicker or more automated (a bit to detect bots would be great)",It increased exponentially over the course of the past few years.
R_1mmwZxu22ySR1Fo,A,1,They have not.,N/A,"Data checks were added to the study prior to administration. If these checks were not passed, bots were suspected.",Data cleaning time increased tremendously. After data cleaning bots were no longer suspected.,Email check: Participant email addresses from suspicious sources ex:@gmx ; Email content from suspected bots tended to contain copy and pasted messages similar across multiple email addresses or contained broken English and native English speakers.,"Use some of the current validated techniques for weeding out bots. Old attention check techniques are not as useful. IP checks were the most effective at finding bots in our studies, but additional validated checks are still needed.",Participants did not have to pass 100% of the bot checks. Real people make mistakes. We allow a small percentage of failed checks depending on the number of checks. 90-95% checks passed has been suggested by some articles.,Additional research assistants to clean data.,NA
R_3O0Lh1XN1CQZapO,A,1,"Bots were compensated automatically, taking away compensation from real potential participants.",Reddit and post on a university-specific subreddit,"Completion time. A survey with over 200 items had completion times of less than 2 minutes. Also, nonsensical answers to open-response items.",It made me lose faith in my data. I ultimately canceled one of the studies because I think bots compromised it. It was disappointing because I am a grad student with limited financial resources for participants.,N/A the responses above captured all the ways I handled bots.,Do not recruit on Amazon's Mturk and be careful with social media recruitment. Be sure to use multiple methods to detect bots and ensure data quality.,"That's a difficult question. For one study, I completely abandoned it because I suspected 1/3 of the data was from bots. For the other study, far less responses were from bots (< 10). I had multiple means to investigate responses, and felt confident that I was removing bots only.",Tools to keep bots from finding my survey in the first place.,NA
R_3s5mXKZ4UUUGf5u,A,1,They have raised it,Just MTURK,One of my undergraduate research assistants found that many participants came from a single IP address (farmers or bots) and many event descriptions were copied from the internet or they made no sense because the robots or farmers did not understand the questions,"They stole money, which then had to come out of my pocket, and they slowed our new collection process because we had to eliminate many non-human participants",We use captcha and we ask participants to name a picture of an eggplant,Talk to people and put in protections before the study begins.  Pay participants $2-3 per hour for experiments at a minimum,"If we have doubts about the data, we remove it.  If you are talking about removing real participants during recruitment, we err on the side of bringing in bots if the conclusion is not clear as we can always exclude the data later",New technology would be great.  I saw code skipping in the tools to minimize cheating,I wonder how many people who obtained reliable data for survey development and validation (and any other types of studies using questionnaires) did not check IP addresses because their data looked good.
R_2xE4LU52aQHQS0h,A,1,"Well, they try to get money from me - and if I don't know they are a bot, they just might succeed.","Facebook, Twitter, LinkedIn, Messenger, Instagram","I had a huge surge in responses, geotagged in regions where we had not engaged in heavy recruitment. I was collecting time zone information and availability for interviews, and noticed discrepancies in the geotagged location and the time zone indicated by the respondent. I noticed some weird patterns in the emails too: They were like FirstLastXYZ@yahoo.com (matching to the first and last name provided, and then three rando a** characters). There were also lots of exact redundant open ended responses, nonsensical responses, etc.","Well, first, it was heartbreaking. This is my dissertation study, and so I was SO excited that it was off to a good start, only to find out that it wasn't. I was initially planning focus groups, but with that many bots, I could not schedule a group and be confident that I would have a minimum number of participants. So I had to screen the data for all the red flags I mentioned above, figure out who I could confidently rule out as a bot, and then reach out to the hundreds of other respondents to see if they were real people. I got like 2 answers back out of hundreds. In the meantime, I had real, human respondents who had agreed to participate. I had to shift gears to individual interviews or risk changes in their eligibility/interest while I figured the rest out. So basically, it affected literally everything about my study. And it was SO disheartening.","No, that's it. But I will add that this was extra frustrating because I approached at least 5 purveyors (cloud research, for example) - but because this was SUCH a niche population, they wouldn't even give me a quote.",NA,"The data used for analysis in my study came from Zoom-based interviews, so I got to be face to face with human people to get my interview data. It is 100% possible that I excluded real people, and if that happened, I am so sorry for not giving them the chance to tell me their story and have it included. I would have really loved to hear from them.","It would be really great if there was a company that just offered a bounce through to verify human-ness... I don't know if that's a thing. I know all the firms guarantee that they screen the data and they have their proprietary checks, but you have to buy the whole package. It would be nice if Qualtrics or other survey platforms added in like a 2-step verification that they could guarantee participants would not give their contact info to the researcher... I don't know. A Turing test? But I've chatted with ChatGPT, and I'm honestly not convinced it wouldn't pass....","I worry that they are becoming lucrative enough to invite human trafficking into the picture. I read something about giant offices like call centers where they employ humans to just jump through the verification hoops and then let the bots do the rest, so they can rack up the gift cards, and that got me wondering whether these people are there because it's a job, or because they are being exploited (possibly varies by situation). But like I said, this breaks my freaking heart."
R_1MQfkb5rd3Sugrt,A,1,"i assume none, because i already have bot checks/filters in place",NA,"response times too fast, straightlining","none as they were removed, and replacement participants were recruited",NA,"use free response boxes that have a specific answer (e.g., reverse a set of words)",the 'anti-bot' questions we ask can be done by any human adult with reasonable reading comprehension. any adult who fails the questions is probably not able to understand the survey questions anyway so would give invalid data. so we don't see a tradeoff here - the human data that is 'accidentally' removed is not valid data in the first place.,survey software/platform needs the ability to have conditionals/filters/etc (like what qualtrics has),NA
R_qUGdMtm1atjpczf,A,1,None to my knowledge. We have a comprehensive screening process for data,"Reddit, twitter","Attention check items, captchas, patterns in responses",Not major due to checklist,NA,NA,"Sending an email to the email provided and letting them know their data was deemed unusable. If we got a reply back, we investigated it more. Also looking at handwriting for signing informed consent - several incredibly similar penmanship strokes without any other possible explanation (very distinct lettering)",Standardized checklist items,NA
R_2Cyv3xYSOgySjcU,A,1,Bots have not impacted compensation,Twitter. To the general public,"The massive influx in respondents, also the highly improbable demographic of some of the responses",Made data cleaning a tedious process,"Age of participant did not match year in program (e.g., multiple 18 yr olds in the 6th yr of their PhD program)","I have only encountered it in one study, but it was a study I later made mutiple links for so the boys did not impact my other links. I would recommend open ended questions to catch for bots. Do not automatically grant compensation, and include attention checks that are easy for humans to respond to but not bots per se","I coded my data into real (verified), probably real, suspicious and bots. Through additional recruitment I was able to make sure the number of verifiably real participants was the largest number. I was also able to do this as this was a second round data collection and compensation was not a part of the second round. If I was suspicious of data I did not include it knowing that I might have let real data go",Maybe put compensation in the flier but not in the text near the survey link. Include attention checks.,Great study! Keep up the good work and the research from this will help a lot of researchers in the future. I think also including a section about how to handle the IRB as it relates to compensating or not compensating bots might help. Luckily many of the bots did not complete the % of the survey they would have needed to to obtain compensation (as stated in my IRB) but with out that protection it could have been a messy situation
R_AdpANChOhasbIid,A,1,"I still feel strongly about compensating participants in all of my studies, but we did lose around $50 to bots in a previous study. They provided emails like all other participants and we manually emailed them gift card codes until we discovered they were bots and learned how to identify them.","Facebook, Twitter, Instagram, our team posted on our individual pages and invited viewers to share the post to their own pages/groups","I noticed there was a clear timing pattern, where I was getting sets of 5 responses every 3 to 5 minutes. Then I started noticing the open-ended responses from these participants didn‚Äôt make sense","They slowed things down dramatically, since we had to take time to remove them from our data and implement protections for incoming responses. They also cost us some of our funding.",These are all of the tactics we used.,NA,"This is challenging. We have to acknowledge that this is a possibility in any publication. We also contacted our participants again for the second round of this data collection, and that seemed to help verify our removal/retention of bots/actual participants because we requested an email from the participants to continue participating.",I think just more training on how to prevent this issue to begin with.,Nope! Thank you!
R_1hZ54hEILhctKaf,A,1,"For the few studies that have used MTurk (I don't typically use online data collection, in part because of bots), I used ""gold-standard"" catch questions to try to see who was paying attention (and potentially pick up on bots). However, after withholding payments on account of failure to answer those questions accurately (and getting pushback on worker message boards), I just ended up paying everyone.",MTurk,Inaccurate responses to gold-standard catch questions,I had to pay bots and time,N/A,Take online data with a grain of salt,"We recognized that we might've been eliminating participants who just failed to pay attention, but we felt comfortable with that decision since it was only a norming study to select stimuli",Not sure that I'll use online studies,N/A
R_vcB2yQHtq8N4ayJ,A,1,Many of the studies have been over MTurk and they receive payment through that website. We state that if the data are unusable due to bots then they will not receive compensation but we feel fairly certain that bots get through.,MTurk,One study had a writing component for sexual minority women and we received,Concerns over data validity and a lot of extra time put in to screening them out,NA,Not to use Mturk! Unless screening them over zoom,"We have chosen to err on the side of being conservative (i.e., having a standard way of determining if we will include a participant or not)",More money for compensation and a more effective way of recruiting participants.,NA
R_1mxUk2OVKGiIOAT,A,1,Need to pay for additional participants,mturk,Answering a direct question with the definition of one of the words in the question rather than an example,Needed to pay additional participants,Ask participants what the study is about as the final question,Always read your data as it comes in,"Have many open ended questions and only include data for which those make sense. Also will be including a question at the very end asking if participants used AI in their responses, as well as using a panel service",Panel service,n/a
R_2yqsfiqsq0dEHse,A,1,Waste of incentive funding with federal grant.,Facebook private groups,Rapidly increasing response rate that wasn't realistic,Took months to determine real from fake respondents; increased time for paying incentives; longer data cleaning time.,NA,NA,Do best we can to identify real from fake using a variety of methods/techniques.,Not sure,NA
R_2rIQ9knzQlVGyZs,A,1,We unintentionally compensated bots prior to realizing the online survey was infiltrated by bots.,"Facebook, Reddit","We hit the recruitment numbers we had been hoping for very fast. In reviewing why that was, we realized the online survey had been infiltrated by bots.","Took longer to clean the data, required more time of researchers on the project to figure out ways to prevent bots, edit the survey accordingly, data cleaning required more hands on support for 4 extra months.",NA,NA,"For any respondents that we are unsure of, we asked for follow-up responses to determine if they were real participants or bots.",A bot preventer...it doesn't presently exist.,NA
R_u2CIEMpcRhUP89H,A,1,NA,Twitter,Noticed odd text responses,Had to take lots of time reviewing surveys to weed out bots,Created form based upon common bot trends and had each participant checked by 2 people,Create a form to systematically review responses,Have cutoff for number of accepted errors,NA,NA
R_1E4GaVdx0vQKD5D,A,1,we had to put additional checks to ensure bots were not being compensated,"Reddit, Twitter",high number of responses with implausible data or too fast response times,had to add additional checks,NA,NA,NA,NA,NA
R_1PYwqI6e9G69o4P,A,1,They haven't yet,"Facebook, Instagram, and Twitter",I could tell from some open-ended responses as well as the time taken to complete the study,"Negative impact because I had over 600 participants but when cleaning the data came to the conclusion that the real responses were under 60 participants so not only did it take a lot of time to clean the data, but the sample size got significantly reduced.",N/A,NA,It's a hard issue to navigate especially when you're not absolutely sure if a subject is a bot or not. Then you have to tangle with the issue of thinking about taking them out or not and the consequences that come out with that decision. I'm still trying to see the best way to balance that.,"Since captcha and some other strategies don't seem to work anymore, it would be great to have some type of resource put into the survey itself or the system where it resides so bots can't get in in the first place. I will not be posting studies on Twitter again, that's where all the bots came from.",It's hard to think about missing one or two bots and leaving them in the data having that doubt about some answers being bots and adding bias to the sample. Immediate resources to tackle this problem should be better addressed because I feel like not a lot of researchers and professionals address it and deal with it how they can but an overall resource to help researchers would be great to assist researchers out there.
R_1LUUVnZvUbBYZz8,A,1,participants who do not pass bot and attention checks are not compensated,n/a,checked bot and attention checks imbedded in my survey,"annoying, potential for skewing results if missed",n/a,include open ended response questions,recruit a larger number of participants,resources to pay for advanced data cleaning,n/a
R_z9bfvUlMdQrtwD7,A,1,"My most recent study, only 100ish participants filled out the main study but 300+ filled out the compensation survey",none for this study,Analyzing qualitative data and getting nonsensical answers,lost participants as a result,Open-ended questions at the end where participants describe what the study was about,Use a service that helps you screen your data. So I now use CloudResearch whenever I want to do a study on MTurk,"If there is a doubt, I remove the data","CloudResearch, and I have heard of other good ones, as well",They are a headache
R_2pLXT6AEL4O8M6d,A,1,We now have a separate questionnaire that participants complete to be compesated.,Physician Source,"The same IP Address was used multiple times, but with the same data and no qualitative entries",Reduced the number of participants and made me suspect of the data,NA,Utilize questions that a BOT cannot answer easily multiple times.,"We look at IP address and if the same data comes from the same IP address and looks exactly the same, we assume either it is a bot or a duplicate that can be removed.  Also, I always use open-ended questions somewhere where participants type the statement, ""I am not a bot"".",Better technology that pulls out human behavior from bot behavior,No
R_6Ezf3AKvxcUJCaB,A,1,"Most of the studies I've run offer a raffle/drawing for incentives, so we've done the same kind of cleaning techniques on the raffle/drawing dataset as on the survey dataset, so I don't think we've paid any bots.","For that study, the dissertation student posted the call for participants on her Instagram account, where she has a large following.","We posted the call for participants and within two days, there were over 4,000 responses. The student was very excited, but I (as her dissertation chair) immediately felt like that was impossible. I then examined the dataset and realized that SO MANY of the responses were non-sensical (I've noticed that many open-ended responses are non-sensical, combinations of variables are implausible, etc.).","They made it so that the student spent weeks cleaning the bots out of the dataset, and were felt less confident in the data that we ended up keeping.","we've used latitude and longitude data to flag potential duplicates for further review of their responses. we've also identified periods of time when most bots came in and reviewed all of those responses in that window with a fine-toothed comb, often one-by-one.","Unfortunately, I am less likely to offer incentives, even in raffle format. I feel very conflicted about this since most of my research asks marginalized respondents to disclose sensitive information. I also recommend utilizing all of the bot detection features available in survey software--I begged our university to add this to our Qualtrics subscription. I also recommend avoiding social media posts unless those posts are in relatively small groups. I've started recruiting more in college classrooms, which is a practice I think we should all move away from, but one where we're less likely to see bots. Finally, screen all responses closely--run all possible bot checks before doing any data analysis and before ending recruitment.","This has often come down to the sample size--what do we need and how conservative can I afford to be in removing what may be human data. Particularly for dissertation student projects, they are often on a time and financial budget that may make me a little less conservative; for my own work, I'm likely to be more conservative (because I have tenure and I'm less dependent on churning out publications, particularly in a teaching focused position).","I would love more information about best practices in removing bots. I feel like they started showing up all of a sudden and I've been scrambling to pull together a set of best practices, particularly as I support dissertation students.",Thank you for doing this survey! This is so frustrating and makes me want to avoid doing online survey research.
R_1kId8D5QH6xWOLM,A,1,None yet because I identify them,"Twitter, Facebook","Email was weird, responses to my emails were odd and didn‚Äôt make sense",Just impacted recruitment. One time I had an influx of 70 bots take my screener survey and I knew it was bots because it was all gmail names and gave similar answers.,Na,To not post studies on social media,Emailing them to set up a phone call or show me a picture of their university ID (for an undergrad study),Not sure,Na
R_2zcqoy7OcXLHCuY,A,1,"I run mostly in-person studies, so it's only affected me slightly--I've had to seek higher levels of funding in order to compensate for possible bots",MTurk,Unusual response patterns,It cost me more money and slowed me down,N/A,In-person studies are the only way to avoid bots entirely,Pre-register,Lots more money and time,N/A
R_2XjTOhDkZ3azRas,A,1,"I hope none, but no guarantee. If anything it‚Äôs sending money into the either with electronic gift cards.",Public groups on Facebook; general Twitter post,Quick succession of responses with weird geolocation and emails listed in the open ended piece.,Mostly challenge in data cleaning,Changing the survey link multiple times and regularly pausing recruitment for a day to reopen the next day.,To watch for them. Adding in all the checks helps with data cleaning but it seems inevitable that bots will access your survey even with all the built-in checks.,We had two people clean the data separately and then compare which ones were considered real to come to consensus.,"More clear guidance on what to do. We spent days learning about prevention measures but there is not consistent work on steps the way there is for missing data, for example.",NA
